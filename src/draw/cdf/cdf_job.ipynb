{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data_path = \"hdfs://10.1.4.11:9000/user/hduser/\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"TraceAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read batck_task parquet\n",
    "df_batch_task = spark\\\n",
    "    .read\\\n",
    "    .parquet(data_path + \"batch_task_parquet\")\n",
    "print(\"batch_task:\")\n",
    "df_batch_task.show()\n",
    "df_batch_task.createOrReplaceTempView(\"batch_task\")\n",
    "\n",
    "# Read batch_instance parquet\n",
    "df_batch_instance = spark\\\n",
    "    .read\\\n",
    "    .parquet(data_path + \"batch_instance_parquet\")\n",
    "print(\"batch_instance:\")\n",
    "df_batch_instance.show()\n",
    "df_batch_instance.createOrReplaceTempView(\"batch_instance\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "######################## Write staging results to HDFS ####################################\n",
    "\n",
    "# # Write job total time to staging results\n",
    "# df_batch_task = spark.sql(\"SELECT job_name, SUM(end_time - start_time) AS duration FROM batch_task GROUP BY job_name\")\n",
    "# df_batch_task.write.parquet(\"batch_task_staging/job_duration_parquet\")\n",
    "# \n",
    "# # Write task total time to staging results\n",
    "# df_batch_task = spark.sql(\"SELECT task_name, SUM(end_time - start_time) AS duration FROM batch_task GROUP BY task_name\")\n",
    "# df_batch_task.write.parquet(\"batch_task_staging/task_duration_parquet\")\n",
    "# \n",
    "# # Write instance total time to staging results\n",
    "# df_batch_instance = spark.sql(\"SELECT instance_name, SUM(end_time - start_time) AS duration FROM batch_instance GROUP BY instance_name\")\n",
    "# df_batch_instance.write.parquet(\"batch_instance_staging/instance_duration_parquet\")\n",
    "\n",
    "###########################################################################################\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a10f0f7eea82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Read staging results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_task_staging/job_duration_parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"duration >= 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Reshape the list to 1-d array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "######################## Write staging results to HDFS ####################################\n",
    "\n",
    "# Read staging results\n",
    "duration = spark.read.parquet(\"batch_task_staging/job_duration_parquet\").filter(\"duration >= 0\")\n",
    "\n",
    "# Reshape the list to 1-d array\n",
    "rows = duration.count()\n",
    "data = np.reshape(duration.select(\"duration\").collect(), rows)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute ecdf and values of x and y\n",
    "ecdf = sm.distributions.ECDF(data)\n",
    "x = np.linspace(min(data), max(data))\n",
    "y = ecdf(x)\n",
    "\n",
    "# Get the first index of x when y = 0.99\n",
    "x_99 = np.argwhere(y >= 0.99)[0][0]\n",
    "\n",
    "# Set layout\n",
    "plt.figure(figsize=[11, 4])\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Duration(seconds)\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.xlim(0.5, 10 ** 6 + 100)\n",
    "plt.ylim(0, 1.01)\n",
    "# plt.xticks(10 ** np.arange(0, 7))\n",
    "# plt.yticks(np.arange(0, 1.2, 0.2))\n",
    "\n",
    "ax = plt.gca()\n",
    "# ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "ax.set_yticklabels([\"{:.0f}%\".format(y * 100) for y in ax.get_yticks()])\n",
    "\n",
    "# Plot\n",
    "plt.plot(x, y, label=\"job\")\n",
    "plt.vlines(x_99, 0, 0.99, colors=\"r\", linestyles=\"dashed\", label=\"99% job\")\n",
    "plt.text(x_99, 0.99, \"101\", size=18, position=(x_99, 0.5))\n",
    "plt.legend(loc='center right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "task_duration = spark.read.parquet(\"batch_task_staging/task_duration_parquet\")\n",
    "ins_duration = spark.read.parquet(\"batch_instance_staging/instance_duration\")\n",
    "\n",
    "# Reshape the list to 1-d array\n",
    "job_rows = job_duration.count()\n",
    "task_rows = task_duration.count()\n",
    "ins_rows = ins_duration.count()\n",
    "\n",
    "job_data = np.reshape(job_duration.select(\"duration\").collect(), job_rows)\n",
    "task_data = np.reshape(task_duration.select(\"duration\").collect(), task_rows)\n",
    "ins_data = np.reshape(ins_duration.select(\"duration\").collect, ins_rows)\n",
    "\n",
    "\n",
    "# Compute ecdf and values of x and y\n",
    "job_ecdf = sm.distributions.ECDF(job_data)\n",
    "task_ecdf = sm.distributions.ECDF(task_data)\n",
    "ins_ecdf = sm.distributions.ECDF(ins_data)\n",
    "\n",
    "job_x = np.linspace(min(job_ecdf), max(job_ecdf))\n",
    "job_y = job_ecdf(job_x)\n",
    "job_99 =\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_container_meta = df_container_meta\\\n",
    "    .select(\"container_id\", \"app_du\")\\\n",
    "    .dropDuplicates()\\\n",
    "    .groupBy(\"app_du\")\\\n",
    "    .agg({\"app_du\": \"count\"})\n",
    "\n",
    "df_container_meta.show()\n",
    "\n",
    "# Reshape the list to 1-d array\n",
    "rows = df_container_meta.count()\n",
    "data = np.reshape(df_container_meta.select(\"count(app_du)\").collect(), rows)\n",
    "\n",
    "# Compute ecdf and values of x and y\n",
    "ecdf = sm.distributions.ECDF(data)\n",
    "x = np.linspace(min(data), max(data))\n",
    "y = ecdf(x)\n",
    "\n",
    "# Set layout\n",
    "plt.figure(figsize=[11, 4])\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Container\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.xlim(1, 600)\n",
    "plt.ylim(0, 1.01)\n",
    "plt.xticks(2 ** np.arange(10))\n",
    "plt.yticks(np.arange(0, 1.2, 0.2))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "ax.set_yticklabels([\"{:.0f}%\".format(y * 100) for y in ax.get_yticks()])\n",
    "\n",
    "# Plot\n",
    "plt.plot(x, y, label=\"service\")\n",
    "plt.vlines(101, 0, 0.99, colors=\"r\", linestyles=\"dashed\", label=\"99% service\")\n",
    "plt.text(101, 0.99, \"101\", size=18, position=(101, 0.5))\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}